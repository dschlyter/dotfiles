#!/usr/bin/env python3

import argparse
import json
import math
import os
import subprocess
import shutil
import sys
from pathlib import Path
from collections import defaultdict
from types import SimpleNamespace

from dotlib.conf import Conf
from dotlib.jstore import JStore
from dotlib.util import print_table

# huge commits are probably autogenerated code or formatting, does not count!
# raw data is stored uncapped, so this can be tweaked later
# this is per language in the commit
MAX_LOC_PER_COMMIT=500

DEFAULTS = os.path.join(Path.home(), ".config", "gloc_defaults.json")

APP_NAME="gloc"
conf = Conf(APP_NAME)
store = JStore(APP_NAME)
DOMAIN = conf.cursor("domain")

# TODO left to fix
#   fix so updates are skipped on old repos

# Data model:
# - There are different domains, typically a domain is captured per machine. A domain must be set before data can be dumped.
# - For every domain, repos are unique per name. Capturing will overwrite database with the same name.
# - For every repo the master branch will be captured, and additions per 

ctx = SimpleNamespace(
    target_author=None,
    metric="added",
    lang=None,
    dates=None,
)

def main():
    # https://docs.python.org/3/library/argparse.html
    global_parser = argparse.ArgumentParser(description='Utility to calculate locs added to git repos. Can calculate for current repo "." or for the local db after running "dump".')
    global_parser.set_defaults(handler=lambda *args, **kwargs: global_parser.print_help())

    global_parser.add_argument('--author', help='email of target author (only when dumping or reading from disk), default to you')
    global_parser.add_argument('-a', '--all', help='show all authors, not just you', action='store_true')
    global_parser.add_argument('--deleted', help='show deleted lines instead of added', action='store_true')
    global_parser.add_argument('--lang', help='only show a specific language')
    global_parser.add_argument('--dates', help='date prefix filter like 2022-10, or a range 2022-01,2022-07')
    subs = global_parser.add_subparsers()

    sp = subs.add_parser('show', help='draw a graph over contributions')
    sp.set_defaults(handler=show)
    sp.add_argument('db_or_repo', nargs="?", help='directory of repo database, or directory of a single git repo')
    sp.add_argument('-q', '--quarters', help='show quarters instead of years', action='store_true')
    sp.add_argument('-m', '--months', help='show months instead of years', action='store_true')
    sp.add_argument('-l', '--log-scale', help='show log scale', action='store_true')

    sp = subs.add_parser('time', help='print lines and agg per year/quarter/month')
    sp.set_defaults(handler=time_cmd)
    sp.add_argument('db_or_repo', nargs="?", help='directory of repo database, or directory of a single git repo')
    sp.add_argument('-f', '--full', help='show full commits, with lines per commit', action='store_true')

    sp = subs.add_parser('langs', help='stats over which languages (filetypes) commited')
    sp.set_defaults(handler=langs)
    sp.add_argument('db_or_repo', nargs="?", help='directory of repo database, or directory of a single git repo')
    sp.add_argument('-y', '--years', help='show yearly breakdown', action='store_true')
    sp.add_argument('-q', '--quarters', help='show quarterly breakdown', action='store_true')
    sp.add_argument('-m', '--months', help='show monthly breakdown', action='store_true')

    sp = subs.add_parser('repos', help='list all repos in database with loc count')
    sp.set_defaults(handler=repos)
    sp.add_argument('db_or_repo', nargs="?", help='directory of repo database, or directory of a single git repo')
    sp.add_argument('-y', '--years', help='show yearly breakdown', action='store_true')
    sp.add_argument('-q', '--quarters', help='show quarterly breakdown', action='store_true')
    sp.add_argument('-m', '--months', help='show monthly breakdown', action='store_true')
    sp.add_argument('-l', '--langs', help='show language usage breakdown', action='store_true')

    sp = subs.add_parser('list', help='list without any data')
    sp.set_defaults(handler=list_cmd)

    sp = subs.add_parser('dump', help='dump stats for all repositories in listed directories to the database')
    sp.set_defaults(handler=dump)
    sp.add_argument('repo_dirs', nargs='+', help='directories of repositories, comma separated')
    sp.add_argument('-d', '--depth', type=int, help='depth of recursion', default=2)
    sp.add_argument('--no-cache', help='update everything', action='store_true')

    sp = subs.add_parser('domain', help='view or set domain')
    sp.set_defaults(handler=domain)
    sp.add_argument('new_domain', nargs="?", help='change to new domain')

    args = global_parser.parse_args()
    if args.all:
        ctx.target_author = None
    elif args.author:
        ctx.target_author = args.author
    else:
        ctx.target_author = sh_read("git config --get user.email")
    if args.deleted:
        ctx.metric = "deleted"
    if args.lang:
        ctx.lang = args.lang
    if args.dates:
        ctx.dates = args.dates
    args.handler(**args.__dict__)


def show(db_or_repo, quarters, months, log_scale, **kwargs):
    data = load_repo_or_db(db_or_repo)

    data = union_repos(data)
    data = sum_langs(data)

    if quarters:
        fn = group_by_quarter
    elif months:
        fn = group_by_month
    else:
        fn = group_by_year
    agg = collapse_keys(fn(data))

    print("Total lines of code:", sum(agg.values()))
    graph_dict(agg, log_scale=log_scale)


def time_cmd(db_or_repo, full, **kwargs):
    data = load_repo_or_db(db_or_repo)

    data = union_repos(data)
    data = sum_langs(data)

    if full:
        print("All commits:")
        print_dict(collapse_keys(data))
        print()
    print("By month:")
    print_dict(collapse_keys(group_by_month(data)))
    print()
    print("By quarter:")
    print_dict(collapse_keys(group_by_quarter(data)))
    print()
    print("By year:")
    print_dict(collapse_keys(group_by_year(data)))
    print()
    print("Total:")
    print_dict(collapse_keys(group_by_all_time(data)))


def langs(db_or_repo, years, quarters, months, **kwargs):
    data = load_repo_or_db(db_or_repo)

    data = union_repos(data)
    if years or quarters or months:
        if years:
            data = group_by_year(data)
        elif quarters:
            data = group_by_quarter(data)
        elif months:
            data = group_by_month(data)
        data = collapse_to_table(data, transpose=True, reverse_cols=True)
        print_table(data, fit=True)
    else:
        data = group_by_anytime(data)
        data = collapse_keys(data)

        res = [(loc, name) for name, loc in data.items()]
        print_table(sorted(res))


def repos(db_or_repo, years, quarters, months, langs, **kwargs):
    data = load_repo_or_db(db_or_repo)

    if years or quarters or months:
        data = sum_langs(data)
        if years:
            data = group_by_year(data)
        elif quarters:
            data = group_by_quarter(data)
        elif months:
            data = group_by_month(data)
        data = collapse_to_table(data, reverse_cols=True)
        print_table(data, fit=True)
    elif langs:
        data = group_by_anytime(data)
        data = collapse_to_table(data, sort_by_colsum=True, reverse_cols=True)
        print_table(data, fit=True)
    else:
        data = sum_langs(data)
        data = group_by_anytime(data)
        data = collapse_keys(data)

        print_table([(loc, name) for name, loc in data.items()])


def list_cmd(**kwargs):
    for key in store.keys():
        print(key)


def domain(new_domain, **kwargs):
    print("Currect domain is", DOMAIN.get())

    if new_domain:
        DOMAIN.put(new_domain)
        print("Updated domain to", DOMAIN.get())

#
# data refinement - basically implementing poor mans pandas :D
#

# flatten repo_data and apply cap
# output: {(repo, time, lang): loc}
def refine(repo_data):
    ret = {}
    for repo_name, repo_data in repo_data.items():
        for time, langs, in repo_data[ctx.metric].items():
            for lang, loc in langs.items():
                if ctx.lang and ctx.lang != lang:
                    continue
                if ctx.dates and "," in ctx.dates:
                    a, b = ctx.dates.split(",")
                    if time < a or time > b:
                        continue
                elif ctx.dates:
                    if not time.startswith(ctx.dates):
                        continue
                if loc > MAX_LOC_PER_COMMIT:
                    loc = 0
                key = (repo_name, time, lang)
                ret[key] = max(loc, ret.get(key, 0))
    if not ret:
        print("Filters removed all data")
        sys.exit(1)
    return ret


# aggregate keys in the cube of data
# a column can be "removed" by setting it to None, however it still exists in the same location to allow for chaining
def merge_keys(flat_data, col, key_fn=lambda key: None, agg_fn=max):
    collect = defaultdict(lambda: [])

    for key, loc in flat_data.items():
        key_list = list(key)
        key_list[col] = key_fn(key_list[col])
        new_key = tuple(key_list)
        collect[new_key].append(loc)

    return {key: agg_fn(val) for key, val in collect.items()}


def union_repos(flat_data):
    # when merging repos, we assume that a conflict on time is a duplicate and pick max
    return merge_keys(flat_data, 0, agg_fn=max)


def sum_langs(flat_data):
    return merge_keys(flat_data, 2, agg_fn=sum)


def group_by_date_len(flat_data, key_len):
    return merge_keys(flat_data, 1, key_fn=lambda key: key[0:key_len], agg_fn=sum)


def group_by_month(flat_data):
    return group_by_date_len(flat_data, 7)


def group_by_year(flat_data):
    return group_by_date_len(flat_data, 4)


def group_by_all_time(flat_data):
    return group_by_date_len(flat_data, 0)


def group_by_quarter(flat_data):
    def date_to_quarter(key):
        year = key[0:4]
        month = int(key[5:7])
        quarter = str(math.ceil(month/3))
        return year + "-Q" + quarter
    return merge_keys(flat_data, 1, key_fn=date_to_quarter, agg_fn=sum)


def group_by_anytime(flat_data):
    return merge_keys(flat_data, 1, agg_fn=sum)


# collapse keys and remove None
def collapse_keys(flat_data):
    return {key[0]: value for key, value in collapse_to(flat_data, 1).items()}

def collapse_to(flat_data, key_len):
    ret = {}

    for key, loc in flat_data.items():
        collapsed = tuple(filter(lambda x: x is not None, key))
        if len(collapsed) != key_len:
            raise Exception(f"Expected key collapse of {key} to resolve to {key_len} value, but found {collapsed}")
        if collapsed in ret:
            raise Exception(f"Expected collapsed key {collapsed} to be unique")
        ret[collapsed] = loc

    return ret


def collapse_to_table(flat_data, transpose=False, reverse_rows=False, reverse_cols=False, sort_by_colsum=False):
    collapsed = collapse_to(flat_data, 2)
    if transpose:
        collapsed = {(k[1],k[0]):v for k,v in collapsed.items()}

    rowkeys = sorted({k[0] for k in collapsed}, reverse=reverse_rows)
    colkeys = sorted({k[1] for k in collapsed}, reverse=reverse_cols)

    if sort_by_colsum:
        colsum = {}
        for k,v in collapsed.items():
            colsum[k[1]] = colsum.get(k[1], 0) + v
        colkeys = sorted([k for k in colkeys], key=lambda k: colsum[k], reverse=reverse_cols)

    ret = [[""]]
    # Header row
    for kx in colkeys:
        ret[-1].append(kx)
    for ky in rowkeys:
        ret.append([ky])
        for kx in colkeys:
            ret[-1].append(collapsed.get((ky, kx), 0))
    return ret


#
# file crawling, database and raw format
#

def load_repo_or_db(path=None):
    # if .git then it is a repo. load this single dir on the same format as the db
    if path is not None and os.path.exists(path):
        orig_dir = os.getcwd()
        os.chdir(path)
        data = {path: get_repo_loc()}
        os.chdir(orig_dir)
    else:
        data = load_data(path)
    
    return refine(data)


def load_data(repo_pattern=None):
    ret = {}
    # fetch in a single batch to avoid chatty exchange with the api
    all_data = store.get_all()
    for key in sorted(all_data.keys()):
        data = all_data[key]
        if repo_pattern and repo_pattern not in key:
            continue
        ret[key] = data
    if not ret:
        if repo_pattern:
            print("Did not find any repo matching", repo_pattern)
        else:
            print("No repos in db, please run dump")
        sys.exit(1)
    return ret


def dump(repo_dirs, depth, no_cache, **kwargs):
    domain = DOMAIN.get()
    if not domain:
        print("Please set domain before running dump")
        sys.exit(1)

    # TODO opt with db age
    # db_age = {file.replace(".json", ""): os.stat(os.path.join(db_dir, file)).st_mtime for file in os.listdir(db_dir)}
    db_age = {}

    res = [
        res
        for d in repo_dirs
        for res in crawl(d, db_age, depth=depth, skip_cache=no_cache)
    ]

    res_by_name = {}
    for (name, data) in res:
        # Note: data can be None here if the results are cached
        if name in res_by_name:
            # TODO better reporting for conflicts, exact paths
            print("Found conflict for repo:", name, "(picking longer repo)")
            new_len = len(data or []) 
            old_len = len(res_by_name[name] or [])
            # TODO tiebreaker on length may have issues if some return None when up to date, actually becomes by time
            if new_len < old_len:
                continue
        res_by_name[name] = data

    updates = 0
    for (repo_name, data) in res_by_name.items():
        if data is None:
            continue
        store.put(f"{domain}:{repo_name}", data)
        updates += 1
    print("Updated", updates, "repos")

    repos_not_found = db_age.keys() - res_by_name.keys()
    if repos_not_found:
        print("Warning: Some repos are in db but were not found:", repos_not_found)
        print("You can use 'jau --app gloc delete' to clean up")


def crawl(crawl_dir, db_age, depth=1, skip_cache=False):
    ret = []

    if os.path.exists(os.path.join(crawl_dir, ".git")):
        orig_cwd = os.getcwd()
        os.chdir(crawl_dir)

        try:
            repo_name = os.path.basename(crawl_dir)
            if skip_cache or int(sh_read("git log -1 --pretty=format:%ct")) > db_age.get(repo_name, 0):
                print("Scanning dir", crawl_dir, end="... ")

                data = get_repo_loc()
                ret.append((repo_name, data))
                print("added", len(data), "commits")
            else:
                print("Up to date", crawl_dir)
                ret.append((repo_name, None))
        except Exception as e:
            print("Error in processing: ", e)
            print("Skipping this dir", crawl_dir)
        
        os.chdir(orig_cwd)
    elif depth > 0:
        contents = os.listdir(crawl_dir)
        for c in contents:
            p = os.path.join(crawl_dir, c)
            if os.path.isdir(p):
                for result in crawl(p, db_age, depth=depth-1, skip_cache=skip_cache):
                    ret.append(result)

    return ret


def get_repo_loc():
    added = {}
    deleted = {}

    # TODO maybe some fallback to find main branch (develop, main, etc), but master is good for now

    commit_lines_changed = sh_read("git log master --numstat --pretty='format:COMMIT;;;%ad;;;%ae' --date='format:%Y-%m-%dT%H:%M:%S'")

    commit_time = None
    commit_author = None

    for line in commit_lines_changed.split("\n"):
        if line == "":
            continue
        elif line.startswith("COMMIT;"):
            _, commit_time, commit_author = line.split(";;;")
        elif ctx.target_author is not None and commit_author != ctx.target_author:
            continue
        else:
            first_col = line.split("\t")[0]
            if first_col != "-":
                locs_added = int(first_col)
                locs_deleted = int(line.split("\t")[0])
                file_path = line.split("\t")[2]
                filename = file_path.split("/")[-1]
                # git may show a {old => new} syntax on changes
                filename = filename.replace("}", "")
                ending = filename.split(".")[-1] if ("." in filename and not filename.startswith(".") and not "=>" in filename) else "" 
                ac = added[commit_time] = added.get(commit_time, {})
                ac[ending] = ac.get(ending, 0) + locs_added
                dc = deleted[commit_time] = deleted.get(commit_time, {})
                dc[ending] = dc.get(ending, 0) + locs_deleted

    return {"added": added, "deleted": deleted}

#
# vis utils
#


def print_dict(d):
    for key in sorted(d.keys()):
        print(key, d[key])


def graph_dict(d, log_scale=False):
    cols = int(shutil.get_terminal_size().columns / 2)
    entries = [(key, d[key]) for key in sorted(d.keys())]
    if len(entries) > cols:
        print("Warning: capping", len(entries), "to columns to maximum of", cols)
        entries = entries[-cols:]

    max_val = max([e[1] for e in entries])
    height = 20 if not log_scale else 10

    # print bar-chart by value
    print("Max value:", max_val)
    for h in range(height):
        if not log_scale:
            cap = max_val - max_val * 1/height * (h+1)
        else:
            cap = max_val * 0.5 ** h

        for e in entries:
            if e[1] > cap:
                print("#", end=" ")
            else:
                print(" ", end=" ")
        print()
    
    # print keys vertically
    for i in range(max([len(e[0]) for e in entries])):
        for e in entries:
            if i < len(e[0]):
                print(e[0][i], end=" ")
            else:
                print(" ", end=" ")
        print()

#
# generic utils
#


def sh_read(command):
    return subprocess.check_output(['/bin/bash', '-o', 'pipefail', '-c', command]).decode("utf-8").strip()


if __name__ == '__main__':
    main()

