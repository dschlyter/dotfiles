#!/usr/bin/env python3

import argparse
import json
import math
import os
import subprocess
import shutil
from pathlib import Path
from collections import defaultdict


# huge commits are probably autogenerated code or formatting, does not count!
# raw data is stored uncapped, so this can be tweaked later
MAX_LOC_PER_COMMIT=500


def main():
    # https://docs.python.org/3/library/argparse.html
    parser = argparse.ArgumentParser(description='Utility to calculate locs added to git repos')
    parser.set_defaults(func=lambda args_without_command: parser.print_usage())
    parser.add_argument('--author', help='email of target author, default to you')
    parser.add_argument('-a', '--all', help='show all authors, not just you', action='store_true')
    subs = parser.add_subparsers()

    sp = subs.add_parser('details', help='print the commit details for the current repository')
    sp.set_defaults(func=details)
    sp.add_argument('-f', '--full', help='show full commits, with lines per commit', action='store_true')

    sp = subs.add_parser('show', help='show stats for the current repository')
    sp.add_argument('-q', '--quarters', help='show quarters instead of years', action='store_true')
    sp.add_argument('-m', '--months', help='show months instead of years', action='store_true')
    sp.add_argument('-l', '--log-scale', help='show log scale', action='store_true')
    sp.set_defaults(func=show)

    sp = subs.add_parser('log', help='show stats for all repositories in the database')
    sp.set_defaults(func=log)
    sp.add_argument('database_read_dir', nargs="?", help='directory of repo database, may contain multiple sub-dbs (from different computers)')
    sp.add_argument('-q', '--quarters', help='show quarters instead of years', action='store_true')
    sp.add_argument('-m', '--months', help='show months instead of years', action='store_true')
    sp.add_argument('-l', '--log-scale', help='show log scale', action='store_true')
    sp.add_argument('--save', help='save database for next time', action='store_true')

    sp = subs.add_parser('repos', help='list all repos in database with loc count')
    sp.set_defaults(func=repos)
    sp.add_argument('database_read_dir', nargs="?", help='directory of repo database, may contain multiple sub-dbs (from different computers)')
    sp.add_argument('--save', help='save database for next time', action='store_true')

    sp = subs.add_parser('dump', help='dump stats for all repositories in listed directories to the database')
    sp.set_defaults(func=dump)
    sp.add_argument('database_write_dir', nargs="?", help='directory of repo database')
    sp.add_argument('repo_dirs', nargs='*', help='directories of repositories, comma separated')
    sp.add_argument('-d', '--depth', type=int, help='depth of recursion', default=2)
    sp.add_argument('--no-cache', help='update everything', action='store_true')
    sp.add_argument('--save', help='save database and repos for next time', action='store_true')

    args = parser.parse_args()
    if args.all:
        args.author = None
    elif args.author is None:
        args.author = sh_read("git config --get user.email")
    args.func(args)


def details(args):
    lines_by_date = get_repo_loc(args.author)
    capped = cap_to_max(lines_by_date)

    if args.full:
        print("By day:")
        print_dict(capped)
        print()
    print("By month:")
    print_dict(collapse(capped, 7))
    print()
    print("By quarter:")
    print_dict(collapse_to_quarter(capped))
    print()
    print("By year:")
    print_dict(collapse(capped, 4))


def show(args):
    lines_by_date = get_repo_loc(args.author)
    show_data(args, lines_by_date)


def show_data(args, lines_by_date):
    capped = cap_to_max(lines_by_date)
    if args.quarters:
        agg = collapse_to_quarter(capped)
    elif args.months:
        agg = collapse(capped, 7)
    else:
        agg = collapse(capped, 4)
    graph_dict(agg, log_scale=args.log_scale)
    print("Total lines of code:", sum(capped.values()))


def log(args):
    db_dir = arg_with_default(args, "database_read_dir", args.save)
    data = load_data(db_dir)
    lines_by_date = merge_dicts([commits for (repo, commits) in data])

    show_data(args, lines_by_date)


def repos(args):
    db_dir = arg_with_default(args, "database_read_dir", args.save)
    data = load_data(db_dir)
    for repo_name, commits in sorted(data):
        commit_count = len(commits)
        loc = sum(cap_to_max(commits).values())
        loc_raw = sum(commits.values())

        print(repo_name.ljust(40), 
            (str(commit_count) + " commits").ljust(15), 
            (str(loc) + " loc").ljust(15), 
            ("(" + str(loc_raw) + " uncapped)").ljust(15) if loc_raw > loc else "")


def load_data(db_dir):
    ret = []
    contents = os.listdir(db_dir)
    for c in contents:
        p = os.path.join(db_dir, c)
        if os.path.isdir(p):
            for sub_res in load_data(p):
                ret.append(sub_res)
        else:
            with open(p) as fp:
                ret.append((c, json.load(fp)))
    return ret


def dump(args):
    db_dir = arg_with_default(args, "database_write_dir", args.save)
    repo_dirs = arg_with_default(args, "repo_dirs", args.save)
    author = args.author

    os.makedirs(db_dir, exist_ok=True)

    db_age = {}
    for file in os.listdir(db_dir):
        mtime = os.stat(os.path.join(db_dir, file)).st_mtime
        db_age[file] = mtime

    res = []
    for d in repo_dirs:
        for sub_res in crawl(d, author, db_age, depth=args.depth, skip_cache=args.no_cache):
            res.append(sub_res)

    res_by_name = {}
    for (name, data) in res:
        # Note: data can be None here if the results are cached
        if name in res_by_name:
            print("Found conflict for repo:", name, "(picking longer repo)")
            new_len = len(data or []) 
            old_len = len(res_by_name[name] or [])
            if new_len < old_len:
                continue
        res_by_name[name] = data

    updates = 0
    for (repo_name, data) in res_by_name.items():
        if data is None:
            continue
        with open(os.path.join(db_dir, repo_name), mode='w') as fp:
            json.dump(data, fp)
            updates += 1
    print("Updated", updates, "repos")

    repos_not_found = db_age.keys() - res_by_name.keys()
    if repos_not_found:
        print("Warning: Some repos are in db but were not found:", repos_not_found)


def crawl(crawl_dir, target_author, db_age, depth=1, skip_cache=False):
    ret = []

    if os.path.exists(os.path.join(crawl_dir, ".git")):
        orig_cwd = os.getcwd()
        os.chdir(crawl_dir)

        try:
            repo_name = os.path.basename(crawl_dir)
            if skip_cache or int(sh_read("git log -1 --pretty=format:%ct")) > db_age.get(repo_name, 0):
                print("Scanning dir", crawl_dir)

                data = get_repo_loc(target_author)
                ret.append((repo_name, data))
            else:
                print("Up to date", crawl_dir)
                ret.append((repo_name, None))
        except Exception as e:
            print(e)
            print("Skipping this dir")
        
        os.chdir(orig_cwd)
    elif depth > 0:
        contents = os.listdir(crawl_dir)
        for c in contents:
            p = os.path.join(crawl_dir, c)
            if os.path.isdir(p):
                for result in crawl(p, target_author, db_age, depth=depth-1, skip_cache=skip_cache):
                    ret.append(result)

    return ret


def get_repo_loc(target_author):
    ret = defaultdict(lambda: 0)

    # TODO maybe some fallback to find main branch, but master is good for now
    commit_lines_changed = sh_read("git log master --numstat --pretty='format:COMMIT;;;%ad;;;%ae' --date='format:%Y-%m-%dT%H:%M:%S'")

    commit_time = None
    commit_author = None

    for line in commit_lines_changed.split("\n"):
        if line == "":
            continue
        elif line.startswith("COMMIT;"):
            _, commit_time, commit_author = line.split(";;;")
        elif target_author is not None and commit_author != target_author:
            continue
        else:
            first_col = line.split("\t")[0]
            if first_col != "-":
                locs_added = int(first_col)
                ret[commit_time] += locs_added

    return ret


def cap_to_max(d):
    ret = {}
    for key in d:
        ret[key] = min(d[key], MAX_LOC_PER_COMMIT)
    return ret


# combine two data sets, pick max value on conflict
# the idea is that commits on the exact same second are probably dubs
def merge_dicts(dicts):
    ret = defaultdict(lambda: 0)
    for d in dicts:
        for key, count in d.items():
            ret[key] = max(ret.get(key, 0), count)
    return ret


# shorten keys of a dict[str,int], and sum values
def collapse(d, key_len):
    ret = defaultdict(lambda: 0)
    for key, count in d.items():
        ret[key[0:key_len]] += count
    return ret


# shorten keys of a dict[str,int], and sum values
def collapse_to_quarter(d):
    ret = defaultdict(lambda: 0)
    for key, count in d.items():
        year = key[0:4]
        month = int(key[5:7])
        quarter = str(math.ceil(month/3))
        ret[year + "-Q" + quarter] += count
    return ret


def print_dict(d):
    for key in sorted(d.keys()):
        print(key, d[key])


def graph_dict(d, log_scale=False):
    cols = int(shutil.get_terminal_size((80, 20)).columns / 2)
    entries = [(key, d[key]) for key in sorted(d.keys())]
    if len(entries) > cols:
        print("Warning: capping", len(entries), "to columns to maximum of", cols)
        entries = entries[-cols:]

    max_val = max([e[1] for e in entries])
    height = 20 if not log_scale else 10

    # print bar-chart by value
    print("Max value:", max_val)
    for h in range(height):
        if not log_scale:
            cap = max_val - max_val * 1/height * (h+1)
        else:
            cap = max_val * 0.5 ** h

        for e in entries:
            if e[1] > cap:
                print("#", end=" ")
            else:
                print(" ", end=" ")
        print()
    
    # print keys vertically
    for i in range(max([len(e[0]) for e in entries])):
        for e in entries:
            if i < len(e[0]):
                print(e[0][i], end=" ")
            else:
                print(" ", end=" ")
        print()


def arg_with_default(args, name, save_flag, required=True):
    DEFAULTS = os.path.join(Path.home(), ".config", "gloc_defaults.json")

    arg_value = getattr(args, name) if hasattr(args, name) else None

    if arg_value:
        if save_flag:
            if os.path.exists(DEFAULTS):
                with open(DEFAULTS) as fp:
                    conf = json.load(fp)
            else:
                conf = {}
            conf[name] = arg_value
            with open(DEFAULTS, mode="w") as fp:
                json.dump(conf, fp)
        return arg_value
    else:
        ret = None
        if os.path.exists(DEFAULTS):
            with open(DEFAULTS) as fp:
                conf = json.load(fp)
            ret = conf.get(name)
        if required and ret is None:
            raise Exception("Value "+name+" must be specified")
        return ret


def sh_read(command):
    return subprocess.check_output(['/bin/bash', '-o', 'pipefail', '-c', command]).decode("utf-8").strip()


if __name__ == '__main__':
    main()

