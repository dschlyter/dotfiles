#!/usr/bin/env python3

import argparse
import json
import math
import os
import subprocess
import shutil
import sys
from pathlib import Path
from collections import defaultdict


# huge commits are probably autogenerated code or formatting, does not count!
# raw data is stored uncapped, so this can be tweaked later
# this is per language in the commit
MAX_LOC_PER_COMMIT=500

DEFAULTS = os.path.join(Path.home(), ".config", "gloc_defaults.json")


def main():
    # https://docs.python.org/3/library/argparse.html
    parser = argparse.ArgumentParser(description='Utility to calculate locs added to git repos. Can calculate for current repo "." or for the local db after running "dump".')
    parser.set_defaults(func=lambda args_without_command: parser.print_usage())
    parser.add_argument('--author', help='email of target author, default to you')
    parser.add_argument('-a', '--all', help='show all authors, not just you', action='store_true')
    parser.add_argument('--save', help='save database dir for next time', action='store_true')
    subs = parser.add_subparsers()

    sp = subs.add_parser('show', help='draw a graph over contributions')
    sp.set_defaults(func=show)
    sp.add_argument('db_or_repo', nargs="?", help='directory of repo database, or directory of a single git repo')
    sp.add_argument('-q', '--quarters', help='show quarters instead of years', action='store_true')
    sp.add_argument('-m', '--months', help='show months instead of years', action='store_true')
    sp.add_argument('-l', '--log-scale', help='show log scale', action='store_true')

    sp = subs.add_parser('details', help='print the raw commit details for the current repository, and agg per year/quarter/month')
    sp.set_defaults(func=details)
    sp.add_argument('db_or_repo', nargs="?", help='directory of repo database, or directory of a single git repo')
    sp.add_argument('-f', '--full', help='show full commits, with lines per commit', action='store_true')

    sp = subs.add_parser('langs', help='stats over which languages (filetypes) commited')
    sp.set_defaults(func=langs)
    sp.add_argument('db_or_repo', nargs="?", help='directory of repo database, or directory of a single git repo')

    sp = subs.add_parser('repos', help='list all repos in database with loc count')
    sp.set_defaults(func=repos)
    sp.add_argument('db_or_repo', nargs="?", help='directory of repo database, or directory of a single git repo')

    sp = subs.add_parser('dump', help='dump stats for all repositories in listed directories to the database')
    sp.set_defaults(func=dump)
    sp.add_argument('database_write_dir', nargs="?", help='directory of repo database')
    sp.add_argument('repo_dirs', nargs='*', help='directories of repositories, comma separated')
    sp.add_argument('-d', '--depth', type=int, help='depth of recursion', default=2)
    sp.add_argument('--no-cache', help='update everything', action='store_true')

    args = parser.parse_args()
    if args.all:
        args.author = None
    elif args.author is None:
        args.author = sh_read("git config --get user.email")
    args.func(args)

# architecture:
# first lines are scraped from disk into the raw format {time: {lang: N}}, this format may be saved as json
# then they are flattened to {(date, lang): N}, this also caps max loc per commit


def show(args):
    db_dir = arg_with_default(args, "db_or_repo", args.save)
    data = load_repo_or_db(db_dir, args.author)

    data = union_repos(data)
    data = sum_langs(data)

    if args.quarters:
        fn = group_by_quarter
    elif args.months:
        fn = group_by_month
    else:
        fn = group_by_year
    agg = collapse_keys(fn(data))

    print("Total lines of code:", sum(agg.values()))
    graph_dict(agg, log_scale=args.log_scale)


def details(args):
    db_dir = arg_with_default(args, "db_or_repo", args.save)
    data = load_repo_or_db(db_dir, args.author)

    data = union_repos(data)
    data = sum_langs(data)

    if args.full:
        print("All commits:")
        print_dict(collapse_keys(data))
        print()
    print("By month:")
    print_dict(collapse_keys(group_by_month(data)))
    print()
    print("By quarter:")
    print_dict(collapse_keys(group_by_quarter(data)))
    print()
    print("By year:")
    print_dict(collapse_keys(group_by_year(data)))


def langs(args):
    db_dir = arg_with_default(args, "db_or_repo", args.save)
    data = load_repo_or_db(db_dir, args.author)

    data = union_repos(data)
    data = group_by_anytime(data)
    data = collapse_keys(data)

    print_dict(data)


def repos(args):
    db_dir = arg_with_default(args, "db_or_repo", args.save)
    data = load_repo_or_db(db_dir, args.author)

    data = sum_langs(data)
    data = group_by_anytime(data)
    data = collapse_keys(data)

    for repo_name, loc in data.items():
        print(repo_name.ljust(40), (str(loc) + " loc").ljust(15))

# TODO table with repo and lang
# TODO table with repo and time
# TODO global --lang flag?


#
# data refinement - basically implementing poor mans pandas :D
#

# flatten repo_data and apply cap
# output: {(repo, time, lang): loc}
def refine(repo_data):
    ret = {}
    for repo_name, commits in repo_data.items():
        for time, langs, in commits.items():
            for lang, loc in langs.items():
                key = (repo_name, time, lang)
                if loc > MAX_LOC_PER_COMMIT:
                    loc = 0
                ret[key] = max(loc, ret.get(key, 0))
    return ret


# aggregate keys in the cube of data
# a column can be "removed" by setting it to None, however it still exists in the same location to allow for chaining
def merge_keys(flat_data, col, key_fn=lambda key: None, agg_fn=max):
    collect = defaultdict(lambda: [])

    for key, loc in flat_data.items():
        key_list = list(key)
        key_list[col] = key_fn(key_list[col])
        new_key = tuple(key_list)
        collect[new_key].append(loc)

    return {key: agg_fn(val) for key, val in collect.items()}


def union_repos(flat_data):
    # when merging repos, we assume that a conflict on time is a duplicate and pick max
    return merge_keys(flat_data, 0, agg_fn=max)


def sum_langs(flat_data):
    return merge_keys(flat_data, 2, agg_fn=sum)


def group_by_date_len(flat_data, key_len):
    return merge_keys(flat_data, 1, key_fn=lambda key: key[0:key_len], agg_fn=sum)


def group_by_month(flat_data):
    return group_by_date_len(flat_data, 7)


def group_by_year(flat_data):
    return group_by_date_len(flat_data, 4)


def group_by_quarter(flat_data):
    def date_to_quarter(key):
        year = key[0:4]
        month = int(key[5:7])
        quarter = str(math.ceil(month/3))
        return year + "-Q" + quarter
    return merge_keys(flat_data, 1, key_fn=date_to_quarter, agg_fn=sum)


def group_by_anytime(flat_data):
    return merge_keys(flat_data, 1, agg_fn=sum)


# collapse keys and remove None
def collapse_keys(flat_data):
    ret = {}

    for key, loc in flat_data.items():
        collapsed = list(filter(lambda x: x is not None, key))
        if len(collapsed) != 1:
            raise Exception(f"Expected key collapse of {key} to resolve to one value")
        new_key = collapsed[0]
        if new_key in ret:
            raise Exception(f"Expected collapsed key {new_key} to be unique")
        ret[new_key] = loc

    return ret


#
# file crawling, database and raw format
#

def load_repo_or_db(directory, target_author=None):
    # if .git then it is a repo. load this single dir on the same format as the db
    git_dir = os.path.join(directory, ".git")
    if os.path.exists(git_dir):
        orig_dir = os.getcwd()
        os.chdir(directory)
        data = {directory: get_repo_loc(target_author)}
        os.chdir(orig_dir)
    else:
        data = load_data(directory)
    
    return refine(data)


def load_data(db_dir):
    ret = {}
    for repo_db_file in os.listdir(db_dir):
        p = os.path.join(db_dir, repo_db_file)
        if os.path.isdir(p):
            for repo, sub_res in load_data(p).items():
                ret[repo] = sub_res
        else:
            if not p.endswith(".json"):
                print("Expected location", db_dir, "to be db_dir, but contained non json file", p)
                sys.exit(1)
            with open(p) as fp:
                repo_name = repo_db_file.replace(".json", "")
                ret[repo_name] = json.load(fp)
    return ret


def dump(args):
    db_dir = arg_with_default(args, "database_write_dir", args.save)
    repo_dirs = arg_with_default(args, "repo_dirs", args.save)
    author = args.author

    os.makedirs(db_dir, exist_ok=True)

    db_age = {file.replace(".json", ""): os.stat(os.path.join(db_dir, file)).st_mtime for file in os.listdir(db_dir)}

    res = [
        res
        for d in repo_dirs
        for res in crawl(d, author, db_age, depth=args.depth, skip_cache=args.no_cache)
    ]

    res_by_name = {}
    for (name, data) in res:
        # Note: data can be None here if the results are cached
        if name in res_by_name:
            print("Found conflict for repo:", name, "(picking longer repo)")
            new_len = len(data or []) 
            old_len = len(res_by_name[name] or [])
            # TODO tiebreaker on length may have issues if some return None when up to date, actually becomes by time
            if new_len < old_len:
                continue
        res_by_name[name] = data

    updates = 0
    for (repo_name, data) in res_by_name.items():
        if data is None:
            continue
        with open(os.path.join(db_dir, repo_name + ".json"), mode='w') as fp:
            json.dump(data, fp)
            updates += 1
    print("Updated", updates, "repos")

    repos_not_found = db_age.keys() - res_by_name.keys()
    if repos_not_found:
        print("Warning: Some repos are in db but were not found:", repos_not_found)


def crawl(crawl_dir, target_author, db_age, depth=1, skip_cache=False):
    ret = []

    if os.path.exists(os.path.join(crawl_dir, ".git")):
        orig_cwd = os.getcwd()
        os.chdir(crawl_dir)

        try:
            repo_name = os.path.basename(crawl_dir)
            if skip_cache or int(sh_read("git log -1 --pretty=format:%ct")) > db_age.get(repo_name, 0):
                print("Scanning dir", crawl_dir)

                data = get_repo_loc(target_author)
                ret.append((repo_name, data))
            else:
                print("Up to date", crawl_dir)
                ret.append((repo_name, None))
        except Exception as e:
            print(e)
            print("Skipping this dir")
        
        os.chdir(orig_cwd)
    elif depth > 0:
        contents = os.listdir(crawl_dir)
        for c in contents:
            p = os.path.join(crawl_dir, c)
            if os.path.isdir(p):
                for result in crawl(p, target_author, db_age, depth=depth-1, skip_cache=skip_cache):
                    ret.append(result)

    return ret


def get_repo_loc(target_author):
    ret = {}

    # TODO maybe some fallback to find main branch, but master is good for now
    commit_lines_changed = sh_read("git log master --numstat --pretty='format:COMMIT;;;%ad;;;%ae' --date='format:%Y-%m-%dT%H:%M:%S'")

    commit_time = None
    commit_author = None

    for line in commit_lines_changed.split("\n"):
        if line == "":
            continue
        elif line.startswith("COMMIT;"):
            _, commit_time, commit_author = line.split(";;;")
        elif target_author is not None and commit_author != target_author:
            continue
        else:
            first_col = line.split("\t")[0]
            if first_col != "-":
                locs_added = int(first_col)
                filename = line.split("\t")[2]
                ending = filename.split(".")[-1] if ("." in filename and not filename.startswith(".") and not "=>" in filename) else "" 
                ret[commit_time] = ret.get(commit_time, {})
                ret[commit_time][ending] = ret[commit_time].get(ending, 0) + locs_added

    return ret

#
# vis utils
#


def print_dict(d):
    for key in sorted(d.keys()):
        print(key, d[key])


def graph_dict(d, log_scale=False):
    cols = int(shutil.get_terminal_size((80, 20)).columns / 2)
    entries = [(key, d[key]) for key in sorted(d.keys())]
    if len(entries) > cols:
        print("Warning: capping", len(entries), "to columns to maximum of", cols)
        entries = entries[-cols:]

    max_val = max([e[1] for e in entries])
    height = 20 if not log_scale else 10

    # print bar-chart by value
    print("Max value:", max_val)
    for h in range(height):
        if not log_scale:
            cap = max_val - max_val * 1/height * (h+1)
        else:
            cap = max_val * 0.5 ** h

        for e in entries:
            if e[1] > cap:
                print("#", end=" ")
            else:
                print(" ", end=" ")
        print()
    
    # print keys vertically
    for i in range(max([len(e[0]) for e in entries])):
        for e in entries:
            if i < len(e[0]):
                print(e[0][i], end=" ")
            else:
                print(" ", end=" ")
        print()

#
# generic utils
#


def arg_with_default(args, name, save_flag, required=True):
    arg_value = getattr(args, name) if hasattr(args, name) else None

    if arg_value:
        if save_flag:
            if os.path.exists(DEFAULTS):
                with open(DEFAULTS) as fp:
                    conf = json.load(fp)
            else:
                conf = {}
            conf[name] = arg_value
            with open(DEFAULTS, mode="w") as fp:
                json.dump(conf, fp)
        return arg_value
    else:
        ret = None
        if os.path.exists(DEFAULTS):
            with open(DEFAULTS) as fp:
                conf = json.load(fp)
            ret = conf.get(name)
        if required and ret is None:
            raise Exception("Value "+name+" must be specified")
        return ret


def sh_read(command):
    return subprocess.check_output(['/bin/bash', '-o', 'pipefail', '-c', command]).decode("utf-8").strip()


if __name__ == '__main__':
    main()

