#!/usr/bin/env python3

import argparse
import math
import os
import subprocess
import shutil
import sys
from pathlib import Path
from collections import defaultdict
from types import SimpleNamespace

from dotlib.conf import Conf
from dotlib.jstore import JStore
from dotlib.util import print_table

APP_NAME="gloc"
conf = Conf(APP_NAME)
store = JStore(APP_NAME)
DOMAIN = conf.cursor("domain")

ctx = SimpleNamespace(
    target_author=None,
    domain=None,
    metric="added",
    lang=None,
    dates=None,
    max_loc=None,
)

def main():
    # https://docs.python.org/3/library/argparse.html
    global_parser = argparse.ArgumentParser(description='Utility to calculate locs added to git repos. Can calculate for current repo "." or for the local db after running "dump".')
    global_parser.set_defaults(handler=lambda *args, **kwargs: global_parser.print_help())

    global_parser.add_argument('--author', help='email of target author (only when dumping or reading from disk), default to you. Set to "all" for all')
    global_parser.add_argument('-d', '--domain', help='show from domain. Set to "all" for all')
    global_parser.add_argument('--deleted', help='show deleted lines instead of added', action='store_true')
    global_parser.add_argument('--lang', help='only show a specific language')
    global_parser.add_argument('--dates', help='date prefix filter like 2022-10, or a range 2022-01,2022-07')
    global_parser.add_argument('--max-loc', help='ignore when loc is above threshold (prolly autogenerated code)', type=int, default=500)
    subs = global_parser.add_subparsers()

    sp = subs.add_parser('show', help='draw a graph over contributions')
    sp.set_defaults(handler=show)
    sp.add_argument('db_or_repo', nargs="?", help='directory of repo database, or directory of a single git repo')
    sp.add_argument('-q', '--quarters', help='show quarters instead of years', action='store_true')
    sp.add_argument('-m', '--months', help='show months instead of years', action='store_true')
    sp.add_argument('-l', '--log-scale', help='show log scale', action='store_true')

    sp = subs.add_parser('time', help='print lines and agg per year/quarter/month')
    sp.set_defaults(handler=time_cmd)
    sp.add_argument('db_or_repo', nargs="?", help='directory of repo database, or directory of a single git repo')
    sp.add_argument('-f', '--full', help='show full commits, with lines per commit', action='store_true')

    sp = subs.add_parser('langs', help='stats over which languages (filetypes) commited')
    sp.set_defaults(handler=langs)
    sp.add_argument('db_or_repo', nargs="?", help='directory of repo database, or directory of a single git repo')
    sp.add_argument('-y', '--years', help='show yearly breakdown', action='store_true')
    sp.add_argument('-q', '--quarters', help='show quarterly breakdown', action='store_true')
    sp.add_argument('-m', '--months', help='show monthly breakdown', action='store_true')

    sp = subs.add_parser('repos', help='list all repos in database with loc count')
    sp.set_defaults(handler=repos)
    sp.add_argument('db_or_repo', nargs="?", help='directory of repo database, or directory of a single git repo')
    sp.add_argument('-y', '--years', help='show yearly breakdown', action='store_true')
    sp.add_argument('-q', '--quarters', help='show quarterly breakdown', action='store_true')
    sp.add_argument('-m', '--months', help='show monthly breakdown', action='store_true')
    sp.add_argument('-l', '--langs', help='show language usage breakdown', action='store_true')

    sp = subs.add_parser('list', help='list without any data')
    sp.set_defaults(handler=list_cmd)

    sp = subs.add_parser('dump', help='dump stats for all repositories in listed directories to the database')
    sp.set_defaults(handler=dump)
    sp.add_argument('repo_dirs', nargs='+', help='directories of repositories, comma separated')
    sp.add_argument('-d', '--depth', type=int, help='depth of recursion', default=2)
    sp.add_argument('--no-cache', help='update everything', action='store_true')
    sp.add_argument('--force-update', help='force update even if it looks out of date', action='store_true')

    sp = subs.add_parser('domain', help='view or set domain')
    sp.set_defaults(handler=domain)
    sp.add_argument('new_domain', nargs="?", help='change to new domain')
    sp.add_argument('--clear', help='remove domain and default to all domains', action='store_true')

    args = global_parser.parse_args()
    if args.author:
        ctx.target_author = args.author
    else:
        ctx.target_author = sh_read("git config --get user.email")

    if args.domain:
        ctx.domain = args.domain
        if ctx.domain == "all":
            ctx.domain = None
    else:
        ctx.domain = DOMAIN.get()
    
    if args.deleted:
        ctx.metric = "deleted"
    if args.lang:
        ctx.lang = args.lang
    if args.dates:
        ctx.dates = args.dates
    args.handler(**args.__dict__)

# Data model:
# - There are different domains, typically a domain is captured per machine. A domain must be set before data can be dumped.
# - For every domain, repos are unique per name. Capturing will overwrite database with the same name.
# - For every repo the master branch will be captured, and additions per 


def show(db_or_repo, quarters, months, log_scale, **kwargs):
    data = load_repo_or_db(db_or_repo)

    data = union_repos(data)
    data = sum_langs(data)

    if quarters:
        fn = group_by_quarter
    elif months:
        fn = group_by_month
    else:
        fn = group_by_year
    agg = collapse_keys(fn(data))

    print("Total lines of code:", sum(agg.values()))
    graph_dict(agg, log_scale=log_scale)


def time_cmd(db_or_repo, full, **kwargs):
    data = load_repo_or_db(db_or_repo)

    data = union_repos(data)
    data = sum_langs(data)

    if full:
        print("All commits:")
        print_dict(collapse_keys(data))
        print()
    print("By month:")
    print_dict(collapse_keys(group_by_month(data)))
    print()
    print("By quarter:")
    print_dict(collapse_keys(group_by_quarter(data)))
    print()
    print("By year:")
    print_dict(collapse_keys(group_by_year(data)))
    print()
    print("Total:")
    print_dict(collapse_keys(group_by_all_time(data)))


def langs(db_or_repo, years, quarters, months, **kwargs):
    data = load_repo_or_db(db_or_repo)

    data = union_repos(data)
    if years or quarters or months:
        if years:
            data = group_by_year(data)
        elif quarters:
            data = group_by_quarter(data)
        elif months:
            data = group_by_month(data)
        data = collapse_to_table(data, transpose=True, reverse_cols=True)
        print_table(data, fit=True)
    else:
        data = group_by_anytime(data)
        data = collapse_keys(data)

        res = [(loc, name) for name, loc in data.items()]
        print_table(sorted(res))


def repos(db_or_repo, years, quarters, months, langs, **kwargs):
    data = load_repo_or_db(db_or_repo)

    if years or quarters or months:
        data = sum_langs(data)
        if years:
            data = group_by_year(data)
        elif quarters:
            data = group_by_quarter(data)
        elif months:
            data = group_by_month(data)
        data = collapse_to_table(data, reverse_cols=True)
        print_table(data, fit=True)
    elif langs:
        data = group_by_anytime(data)
        data = collapse_to_table(data, sort_by_colsum=True, reverse_cols=True)
        print_table(data, fit=True)
    else:
        data = sum_langs(data)
        data = group_by_anytime(data)
        data = collapse_keys(data)

        print_table([(loc, name) for name, loc in data.items()])


def list_cmd(**kwargs):
    for key in sorted(store.keys()):
        if ctx.domain:
            p = _get_key_prefix(ctx.domain)
            if key.startswith(p):
                print(key.replace(p, ""))
        else:
            print(key)


def domain(new_domain, clear, **kwargs):
    if new_domain:
        DOMAIN.put(new_domain)
    elif clear:
        DOMAIN.put(None)

    for d in sorted({k.split(":")[0] for k in store.keys()} | {str(DOMAIN.get())}):
        print("*" if d == str(DOMAIN.get()) else " ", d)


def _get_key_prefix(domain_name):
    return f"{domain_name}:"

#
# data refinement - basically implementing poor mans pandas :D
#

# flatten repo_data and apply cap
# output: {(repo, time, lang): loc}
def refine(repo_data):
    ret = {}
    for repo_name, repo_data in repo_data.items():
        for time, langs, in repo_data[ctx.metric].items():
            for lang, loc in langs.items():
                if ctx.lang and ctx.lang != lang:
                    continue
                if ctx.dates and "," in ctx.dates:
                    a, b = ctx.dates.split(",")
                    if time < a or time > b:
                        continue
                elif ctx.dates:
                    if not time.startswith(ctx.dates):
                        continue
                if loc > ctx.max_loc:
                    loc = 0
                key = (repo_name, time, lang)
                ret[key] = max(loc, ret.get(key, 0))
    if not ret:
        print("Filters removed all data")
        sys.exit(1)
    return ret


# aggregate keys in the cube of data
# a column can be "removed" by setting it to None, however it still exists in the same location to allow for chaining
def merge_keys(flat_data, col, key_fn=lambda key: None, agg_fn=max):
    collect = defaultdict(lambda: [])

    for key, loc in flat_data.items():
        key_list = list(key)
        key_list[col] = key_fn(key_list[col])
        new_key = tuple(key_list)
        collect[new_key].append(loc)

    return {key: agg_fn(val) for key, val in collect.items()}


def union_repos(flat_data):
    # when merging repos, we assume that a conflict on time is a duplicate and pick max
    return merge_keys(flat_data, 0, agg_fn=max)


def sum_langs(flat_data):
    return merge_keys(flat_data, 2, agg_fn=sum)


def group_by_date_len(flat_data, key_len):
    return merge_keys(flat_data, 1, key_fn=lambda key: key[0:key_len], agg_fn=sum)


def group_by_month(flat_data):
    return group_by_date_len(flat_data, 7)


def group_by_year(flat_data):
    return group_by_date_len(flat_data, 4)


def group_by_all_time(flat_data):
    return group_by_date_len(flat_data, 0)


def group_by_quarter(flat_data):
    def date_to_quarter(key):
        year = key[0:4]
        month = int(key[5:7])
        quarter = str(math.ceil(month/3))
        return year + "-Q" + quarter
    return merge_keys(flat_data, 1, key_fn=date_to_quarter, agg_fn=sum)


def group_by_anytime(flat_data):
    return merge_keys(flat_data, 1, agg_fn=sum)


# collapse keys and remove None
def collapse_keys(flat_data):
    return {key[0]: value for key, value in collapse_to(flat_data, 1).items()}

def collapse_to(flat_data, key_len):
    ret = {}

    for key, loc in flat_data.items():
        collapsed = tuple(filter(lambda x: x is not None, key))
        if len(collapsed) != key_len:
            raise Exception(f"Expected key collapse of {key} to resolve to {key_len} value, but found {collapsed}")
        if collapsed in ret:
            raise Exception(f"Expected collapsed key {collapsed} to be unique")
        ret[collapsed] = loc

    return ret


def collapse_to_table(flat_data, transpose=False, reverse_rows=False, reverse_cols=False, sort_by_colsum=False):
    collapsed = collapse_to(flat_data, 2)
    if transpose:
        collapsed = {(k[1],k[0]):v for k,v in collapsed.items()}

    rowkeys = sorted({k[0] for k in collapsed}, reverse=reverse_rows)
    colkeys = sorted({k[1] for k in collapsed}, reverse=reverse_cols)

    if sort_by_colsum:
        colsum = {}
        for k,v in collapsed.items():
            colsum[k[1]] = colsum.get(k[1], 0) + v
        colkeys = sorted([k for k in colkeys], key=lambda k: colsum[k], reverse=reverse_cols)

    ret = [[""]]
    # Header row
    for kx in colkeys:
        ret[-1].append(kx)
    for ky in rowkeys:
        ret.append([ky])
        for kx in colkeys:
            ret[-1].append(collapsed.get((ky, kx), 0))
    return ret


#
# file crawling, database and raw format
#

def load_repo_or_db(path=None):
    if path is not None and os.path.exists(path):
        res = crawl(path, depth=2, skip_cache=True)
        data = {repo:data for repo,data,abs_path in res}
    else:
        data = load_data(path)
    
    return refine(data)


def load_data(repo_pattern=None):
    ret = {}
    # fetch in a single batch to avoid chatty exchange with the api
    all_data = store.get_all()
    for key in sorted(all_data.keys()):
        data = all_data[key]
        if ctx.domain:
            if not key.startswith(_get_key_prefix(ctx.domain)):
                continue
            else:
                key = key.replace(_get_key_prefix(ctx.domain), "")
        if repo_pattern and repo_pattern not in key:
            continue
        ret[key] = data
    if not ret:
        if repo_pattern:
            print("Did not find any repo matching", repo_pattern)
        else:
            print("No repos in db, please run dump")
        sys.exit(1)
    return ret


def dump(repo_dirs, depth, no_cache, force_update, **kwargs):
    domain = DOMAIN.get()
    if not domain:
        print("Please set domain before running dump")
        sys.exit(1)
    dp = _get_key_prefix(domain)

    existing_db = {k.replace(dp,""):v for k,v in store.get_all().items() if dp in k}

    res = [
        res
        for d in repo_dirs
        for res in crawl(d, existing_db=existing_db, depth=depth, skip_cache=no_cache)
    ]

    grouped = {}
    for r in res:
        grouped[r[0]] = grouped.get(r[0], []) + [r]

    for name,r in grouped.items():
        if len(r) >= 2:
            paths = [v[2] for v in r]
            print(f"Warning! Multiple repos with name {name} {paths}")

    updated_count, up_to_date_count, out_out_date = 0,0,0
    res_by_name = {}
    for (name, data, abs_path) in res:
        if data is None:
            up_to_date_count += 1
            continue
        if name in existing_db:
            new_len = len(data["added"])
            existing_len = len(existing_db[name]["added"])
            if existing_len >= new_len and not force_update:
                print(f"Warning! Skipping out of date {name} at {abs_path} since history not longer than in db: {new_len} <= {existing_len}")
                out_out_date += 1
                continue
        res_by_name[name] = data

    for (repo_name, data) in res_by_name.items():
        store.put(f"{dp}{repo_name}", data)
        updated_count += 1
    print(f"Dump complete! {updated_count} updated, {up_to_date_count} up to date, {out_out_date} out of date")

    already_in_domain = set([k.replace(dp, "") for k in store.keys() if dp in k])
    repos_not_found = already_in_domain - res_by_name.keys()
    if repos_not_found:
        print("Warning: Some repos are in db but were not found:", repos_not_found)
        print("You can use 'jau --app gloc delete' to clean up")


def crawl(crawl_dir, existing_db={}, depth=1, skip_cache=False):
    ret = []

    if os.path.exists(os.path.join(crawl_dir, ".git")):
        print("Scanning dir", crawl_dir, end="... ", flush=True)
        orig_cwd = os.getcwd()
        os.chdir(crawl_dir)

        try:
            abs_path = os.path.abspath(crawl_dir)
            repo_name = os.path.basename(abs_path)
            master_sha = sh_read("git rev-parse master")
            if skip_cache or master_sha != existing_db.get(repo_name, {}).get("master_sha", None):
                data = get_repo_loc()
                data["master_sha"] = master_sha
                if data["added"]:
                    ret.append((repo_name, data, abs_path))
                    print("added", len(data["added"]), "commits")
                else:
                    print("no commits found, skipping")
            else:
                print("db in sync")
                ret.append((repo_name, None, abs_path))
        except Exception as e:
            print("Error in processing: ", e)
            print("Skipping this dir", crawl_dir)
        
        os.chdir(orig_cwd)
    elif depth > 0:
        contents = os.listdir(crawl_dir)
        for c in contents:
            p = os.path.join(crawl_dir, c)
            if os.path.isdir(p):
                for result in crawl(p, existing_db=existing_db, depth=depth-1, skip_cache=skip_cache):
                    ret.append(result)

    return ret


def get_repo_loc():
    added = {}
    deleted = {}

    # TODO maybe some fallback to find main branch (develop, main, etc), but master is good for now

    commit_lines_changed = sh_read("git log master --numstat --pretty='format:COMMIT;;;%ad;;;%ae' --date='format:%Y-%m-%dT%H:%M:%S'")

    commit_time = None
    commit_author = None

    for line in commit_lines_changed.split("\n"):
        if line == "":
            continue
        elif line.startswith("COMMIT;"):
            _, commit_time, commit_author = line.split(";;;")
        elif ctx.target_author != "all" and commit_author != ctx.target_author:
            continue
        else:
            first_col = line.split("\t")[0]
            if first_col != "-":
                locs_added = int(first_col)
                locs_deleted = int(line.split("\t")[1])
                file_path = line.split("\t")[2]
                filename = file_path.split("/")[-1]
                # git may show a {old => new} syntax on changes
                filename = filename.replace("}", "")
                ending = filename.split(".")[-1] if ("." in filename and not filename.startswith(".") and not "=>" in filename) else "" 
                ac = added[commit_time] = added.get(commit_time, {})
                ac[ending] = ac.get(ending, 0) + locs_added
                dc = deleted[commit_time] = deleted.get(commit_time, {})
                dc[ending] = dc.get(ending, 0) + locs_deleted

    return {"added": added, "deleted": deleted}

#
# vis utils
#


def print_dict(d):
    for key in sorted(d.keys()):
        print(key, d[key])


def graph_dict(d, log_scale=False):
    cols = int(shutil.get_terminal_size().columns / 2)
    entries = [(key, d[key]) for key in sorted(d.keys())]
    if len(entries) > cols:
        print("Warning: capping", len(entries), "to columns to maximum of", cols)
        entries = entries[-cols:]

    max_val = max([e[1] for e in entries])
    height = 20 if not log_scale else 10

    # print bar-chart by value
    print("Max value:", max_val)
    for h in range(height):
        if not log_scale:
            cap = max_val - max_val * 1/height * (h+1)
        else:
            cap = max_val * 0.5 ** h

        for e in entries:
            if e[1] > cap:
                print("#", end=" ")
            else:
                print(" ", end=" ")
        print()
    
    # print keys vertically
    for i in range(max([len(e[0]) for e in entries])):
        for e in entries:
            if i < len(e[0]):
                print(e[0][i], end=" ")
            else:
                print(" ", end=" ")
        print()

#
# generic utils
#


def sh_read(command):
    return subprocess.check_output(['/bin/bash', '-o', 'pipefail', '-c', command]).decode("utf-8").strip()


if __name__ == '__main__':
    main()

